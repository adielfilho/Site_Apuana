
<!DOCTYPE html>

<html lang="pt-BR">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Acessando o Cluster &#8212; documentação Cluster Cin latest</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/groundwork.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/translations.js"></script>
    <link rel="index" title="Índice" href="genindex.html" />
    <link rel="search" title="Buscar" href="search.html" />
    <link rel="next" title="O que é um cluster de computadores?" href="TheoryCluster/Theory_cluster.html" />
    <link rel="prev" title="Arquitetura do cluster Apuana" href="cluster_cin.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navegação</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="Índice Geral"
             accesskey="I">índice</a></li>
        <li class="right" >
          <a href="TheoryCluster/Theory_cluster.html" title="O que é um cluster de computadores?"
             accesskey="N">próximo</a> |</li>
        <li class="right" >
          <a href="cluster_cin.html" title="Arquitetura do cluster Apuana"
             accesskey="P">anterior</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">documentação Cluster Cin latest</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Acessando o Cluster</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="acessando-o-cluster">
<span id="h-js2b7t82mute"></span><h1>Acessando o Cluster<a class="headerlink" href="#acessando-o-cluster" title="Link permanente para este cabeçalho">¶</a></h1>
<p>Para acessar o cluster é necessário ter um login cin.ufpe.br e ter o
acesso habilitado as máquinas de acesso do cluster, também é necessário
estar na VPN do CIn.</p>
</section>
<section id="acessando-a-vpn-do-cin">
<h1>Acessando a VPN do CIn<a class="headerlink" href="#acessando-a-vpn-do-cin" title="Link permanente para este cabeçalho">¶</a></h1>
<p>Mesmo fazendo a conexão utilizando a rede interna do CIn, é necessário usar a VPN do CIn. Para maiores informações sobre o uso da VPN do CIn, consultar a documentação disponível na página do HelpDesk para VPN do CIn (<a class="reference external" href="https://helpdesk.cin.ufpe.br/redes/vpn">https://helpdesk.cin.ufpe.br/redes/vpn</a>). O canal para tirar dúvidas sobre conexão com a VPN é através do HelpDesk (<a class="reference external" href="mailto:helpdesk&#37;&#52;&#48;cin&#46;ufpe&#46;br">helpdesk<span>&#64;</span>cin<span>&#46;</span>ufpe<span>&#46;</span>br</a>)</p>
</section>
<section id="conectando-a-maquina-de-acesso">
<h1>Conectando a Maquina de Acesso<a class="headerlink" href="#conectando-a-maquina-de-acesso" title="Link permanente para este cabeçalho">¶</a></h1>
<p>O envio de tarefas para o cluster precisa ser feito a partir de uma
maquina de acesso, a entrada na maquina de acesso é realizado através do
SSH</p>
<p>        ssh &lt;login&gt;&#64;slurm-client1.cin.ufpe.br</p>
<p>Uma vez conectado na máquina de acesso é possível alocar e editar seus
jobs a partir de comandos
<a class="reference external" href="https://www.google.com/url?q=https://slurm.schedmd.com/documentation.html&amp;sa=D&amp;source=editors&amp;ust=1683228690609179&amp;usg=AOvVaw2aSvDJwDiM7wf_WKq0h5lp">slurm</a>.
Primeiramente, você precisará preparar seu ambiente de trabalho no
servidor que você utilizará. Jobs que utilizam máquinas virtuais (Docker
e afins) não serão aprovados, pois abrem brechas de segurança. Qualquer
biblioteca que necessite de instalação, por favor, nos contate e faremos
a instalação o mais rápido possível. Para acessar o servidor que
realizará o processamento, utilize o comando “salloc”, como no exemplo
abaixo:</p>
<p>salloc</p>
<p>Nossos servidores já tem o gerenciador de pacotes
<a class="reference external" href="https://www.google.com/url?q=https://www.anaconda.com/products/distribution&amp;sa=D&amp;source=editors&amp;ust=1683228690610341&amp;usg=AOvVaw3cLVSQY5yUsKcLch94qn4r">conda</a> para
qualquer projeto que for realizado em Python. Os comandos de git também
estão disponíveis e prontos para uso. Após a preparação do seu ambiente,
volte para o nodo de login com o comando “exit” e rode o seu job com o
comando
<a class="reference external" href="https://www.google.com/url?q=https://slurm.schedmd.com/srun.html&amp;sa=D&amp;source=editors&amp;ust=1683228690610939&amp;usg=AOvVaw3QWOdTZiBB1V14-VrnC1a2">srun</a>.
Exemplo:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">srun --gpus=N_GPUS --cpus-per-task=N_CPUS nvidia-smi</span>
</pre></div>
</div>
</section>
<section id="ambientes-virtuais-python">
<span id="h-fnvohiub06p3"></span><h1>Ambientes Virtuais Python<a class="headerlink" href="#ambientes-virtuais-python" title="Link permanente para este cabeçalho">¶</a></h1>
<p>Um ambiente virtual em Python é um ambiente local e isolado no qual você
pode instalar ou desinstalar pacotes Python sem interferir no ambiente
global (ou em outros ambientes virtuais). Geralmente reside em um
diretório. Para usar um ambiente virtual, você deve ativá-lo. Ativar um
ambiente essencialmente define variáveis ​​de ambiente em seu shell para
que:</p>
<ul class="simple">
<li><p>‘python’ aponta para a versão correta do Python para aquele ambiente</p></li>
<li><p>‘python’ procura pelos pacotes no ambiente virual</p></li>
<li><p>‘pip install’ instala pacotes no ambiente virtual</p></li>
<li><p>Qualquer comando shell instalado via ‘pip install’ é disponibilizado</p></li>
</ul>
</section>
<section id="pip-virtualenv">
<span id="h-t03rdi3pn51o"></span><h1>Pip/Virtualenv<a class="headerlink" href="#pip-virtualenv" title="Link permanente para este cabeçalho">¶</a></h1>
<p>Pip é o gerenciador de pacotes preferido para Python e cada cluster
fornece várias versões do Python por meio do módulo associado que vem
com o pip. Para instalar novos pacotes, primeiro você terá que criar um
espaço pessoal para armazená-los. A solução preferida é usar ambientes
virtuais.</p>
<p>Primeiro, carregar o modulo Python que deseja utilizar</p>
<p> module load Python3.10</p>
<p>Modulos Disponiveis</p>
<ul class="simple">
<li><p>Python3.10 -&gt; Python/3.10.8</p></li>
<li><p>Python3.9 -&gt; Python/3.9.6</p></li>
<li><p>Python3.8 -&gt; Python/3.8.6</p></li>
</ul>
<p>Depois crie um ambiente virutal (onde &lt;env&gt; é o nome do ambiente) no seu
diretório home:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">python -m venv $HOME/&lt;env&gt;</span>
</pre></div>
</div>
<p>Para ativar o ambiente criado:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">source $HOME/&lt;env&gt;/bin/activate</span>
</pre></div>
</div>
<p>Você agora pode instalar qualquer pacote Python utilizando o comando
pip, exemplo pytorch:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">pip install torch torchvision</span>
</pre></div>
</div>
<p>Recomendamos que para a criação do ambiente para rodar o seus job,
alocar uma maquina de forma interativa utilizando o comando salloc, nele
você pode criar e testar o ambiente, uma vez criado e funcional, para
utilizar você deve carregar o modulo e ativar o ambiente no seu script.</p>
</section>
<section id="execucao-de-jobs">
<span id="h-elx4sixwhp8c"></span><h1>Execução de jobs<a class="headerlink" href="#execucao-de-jobs" title="Link permanente para este cabeçalho">¶</a></h1>
<p>Ao escrever um script .sh, é necessário ativar o ambiente no início e
instalar os pacotes necessários. Um exemplo de um script com tensorflow
é mostrado a seguir:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">#</span>!/bin/bash
<span class="gp">#</span>SBATCH<span class="w"> </span>--job-name<span class="o">=</span>test_job
<span class="gp">#</span>SBATCH<span class="w"> </span>--ntasks<span class="o">=</span><span class="m">1</span>
<span class="gp">#</span>SBATCH<span class="w"> </span>--mem<span class="w"> </span>16G
<span class="gp">#</span>SBATCH<span class="w"> </span>-c<span class="w"> </span><span class="m">8</span>
<span class="gp">#</span>SBATCH<span class="w"> </span>-o<span class="w"> </span>job.log
<span class="gp">#</span>SBATCH<span class="w"> </span>--output<span class="o">=</span>job_output.txt
<span class="gp">#</span>SBATCH<span class="w"> </span>--error<span class="o">=</span>job_error.txt

<span class="gp"># </span>carregar<span class="w"> </span>versão<span class="w"> </span>python
<span class="go">module load Python/3.10</span>
<span class="gp"># </span>ativar<span class="w"> </span>ambiente
<span class="go">source $HOME/env_teste/bin/activate</span>
<span class="gp"># </span>executar<span class="w"> </span>.py
<span class="go">python $HOME/test_dir/test.py</span>
</pre></div>
</div>
<section id="comandos-basicos-de-gerenciamento-de-jobs">
<span id="h-h11648s05oky"></span><h2>Comandos básicos de gerenciamento de jobs<a class="headerlink" href="#comandos-basicos-de-gerenciamento-de-jobs" title="Link permanente para este cabeçalho">¶</a></h2>
<p>Para agendar o job faça:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">sbatch test_slurm.sh</span>
</pre></div>
</div>
<p>Para verificar erros no job faça (dentro do diretório do arquivo
job_error.txt):</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">cat job_error.txt</span>
</pre></div>
</div>
<p>Para observar os outputs do job faça (dentro do diretório do arquivo
job_output.txt):</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">cat job_output.txt</span>
</pre></div>
</div>
<p>Para verificar a posição do job na fila faça:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">squeue</span>
</pre></div>
</div>
<p>Para cancelar o job faça:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">scancel job_id</span>
</pre></div>
</div>
</section>
<section id="exemplo-com-repositorio-publico-do-github">
<span id="h-p9bbx0k61d7n"></span><span id="h-16ozdksheroh"></span><h2>Exemplo com repositório público do GitHub<a class="headerlink" href="#exemplo-com-repositorio-publico-do-github" title="Link permanente para este cabeçalho">¶</a></h2>
<p>Primeiro é necessário clonar o repositório. Obs.: o diretório home do
usuário é sincronizado entre todas as máquinas.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">git clone https://github.com/username/repoName.git</span>
</pre></div>
</div>
<p>Depois de clonar o repositório, é criado um script .sh. Uma das
alternativas é utilizando nano:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">nano test_slurm.sh</span>
</pre></div>
</div>
<p>Em seguida, o usuário preenche o script com as diretivas do SBATCH que
ele acha necessário e depois com os comandos que devem ser executados no
node. Primeiro, será apresentado um script que faz uso de Pytorch.</p>
<p>test_slurm.sh</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">#</span>!/bin/bash
<span class="gp">#</span>SBATCH<span class="w"> </span>--job-name<span class="o">=</span>test_job
<span class="gp">#</span>SBATCH<span class="w"> </span>--ntasks<span class="o">=</span><span class="m">1</span>
<span class="gp">#</span>SBATCH<span class="w"> </span>--mem<span class="w"> </span>16G
<span class="gp">#</span>SBATCH<span class="w"> </span>-c<span class="w"> </span><span class="m">8</span>
<span class="gp">#</span>SBATCH<span class="w"> </span>-o<span class="w"> </span>job.log
<span class="gp">#</span>SBATCH<span class="w"> </span>--output<span class="o">=</span>job_output.txt
<span class="gp">#</span>SBATCH<span class="w"> </span>--error<span class="o">=</span>job_error.txt

<span class="gp"># </span>carregar<span class="w"> </span>versão<span class="w"> </span>python
<span class="go">module load Python/3.9.6</span>
<span class="gp"># </span>criar<span class="w"> </span>ambiente
<span class="go">python -m venv $HOME/env_teste</span>
<span class="gp"># </span>ativar<span class="w"> </span>ambiente
<span class="go">source $HOME/env_teste/bin/activate</span>
<span class="gp"># </span>instalar<span class="w"> </span>pacotes<span class="w"> </span>desejados
<span class="go">pip install pytorch</span>
<span class="go">pip install pandas</span>
<span class="go">pip install matplotlib</span>
<span class="go">pip install seaborn</span>
<span class="go">Pip install IPython</span>
<span class="gp"># </span>executar<span class="w"> </span>.py
<span class="go">python $HOME/repoName/thisScript.py</span>
</pre></div>
</div>
<p>Perceba a criação de um novo ambiente e em seguida sua ativação. Aqui
foi realizado o dowgrade da versão do Python de 3.10 para 3.9. Isto foi
feito porque ainda existem bugs da classe DataLoader do PyTorch ao
utilizar o python 3.10. O cluster é bastante versátil neste aspecto,
pois pode-se escolher a versão do python (dentre as listadas acima) e
das dependências mais adequadas para o funcionamento do seu código no
ambiente virtual.</p>
</section>
<section id="exemplo-com-repositorio-privado-do-github">
<span id="h-u2s1lukw4mc7"></span><h2>Exemplo com repositório privado do GitHub<a class="headerlink" href="#exemplo-com-repositorio-privado-do-github" title="Link permanente para este cabeçalho">¶</a></h2>
<p>Clonando um repositório privado :</p>
<p>Para clone repositorios privados recomendamos conectar na maquina de
login (slurm-client1) passando adiante o seu agente SSH, permitindo a
você utilizar a chave SSH configurada na sua maquina local na sessão
SSH, para tal é preciso utilizar o parametro -A</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">ssh -A &lt;login&gt;@slurm-client1.cin.ufpe.br</span>
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">git clone git@github.com:username:token@github.com/username/repoName.git</span>
</pre></div>
</div>
</section>
<section id="sincronizacao-de-arquivos-entre-o-cluster-a-sua-maquina">
<span id="h-kbo1jo3m9z40"></span><h2>Sincronização de Arquivos Entre o Cluster a Sua Máquina<a class="headerlink" href="#sincronizacao-de-arquivos-entre-o-cluster-a-sua-maquina" title="Link permanente para este cabeçalho">¶</a></h2>
<p>Para a sincronização/transferência de arquivos entre sua máquina e o
cluster deve ser utilizado o comando rsync
(<a class="reference external" href="https://www.google.com/url?q=https://download.samba.org/pub/rsync/rsync.1&amp;sa=D&amp;source=editors&amp;ust=1683228690625949&amp;usg=AOvVaw1h4cJRq-mEcdnOojWx8lGE">Documentação</a>,
<a class="reference external" href="https://www.google.com/url?q=https://www.digitalocean.com/community/tutorials/how-to-use-rsync-to-sync-local-and-remote-directories-pt&amp;sa=D&amp;source=editors&amp;ust=1683228690626302&amp;usg=AOvVaw2x91BoV8AGqpGJzdYZulSa">Tutorial</a>)
através da diretório home da máquina de login, lembrando que este é
sincronizado com os nós de computação do cluster.</p>
<p>Exemplos:</p>
<ul class="simple">
<li><p>Sincronizando pasta da máquina local para o cluster:
rsync –bwlimit=1000 -azP pasta-1 &lt;login&gt;&#64;slurm-client1.cin.ufpe.br:~</p></li>
<li><p>A pasta de nome “pasta-1” vai ser copiada/sincronizada para a pasta
de nome “pasta-1” no seu diretório home “~/pasta-1”</p></li>
<li><p>O argumento –bwlimit limita a velocidade de transferência, dado em
KBytes por segund</p></li>
<li><p>O argumento -a é para archive mode, sendo equivalente a utilização
dos argumentos “ -rlptgoD “</p></li>
<li><p>O argumento -z realiza a compressão dos dados para transferência</p></li>
<li><p>O argumento -P mostra o progresso da transferência e resume
transferências interrompidas</p></li>
<li><p>Sincronizando pasta do cluster para máquina local:
rsync –bwlimit=1000 -azP &lt;login&gt;&#64;slurm-client1.cin.ufpe.br:~/pasta-1
~</p></li>
<li><p>Mesmo do exemplo anterior, porém a ordem é invertida, a pasta-1 do
diretório home do cluster é copiada para o diretório home local</p></li>
</ul>
<p>A utilização do rsync também é recomendada para transferências dentro da
própria máquina do cluster quando for realizada entre volumes em rede,
como por exemplo transferências do diretório home para o diretório /tmp
local do nó de computação</p>
</section>
</section>
<section id="particoes-e-limites-de-recursos">
<span id="h-6xrj2d7u8h14"></span><h1>Partições e limites de recursos<a class="headerlink" href="#particoes-e-limites-de-recursos" title="Link permanente para este cabeçalho">¶</a></h1>
<p>Atualmente o cluster contém duas partições: long e short. A partição
long é a partição padrão e  é adequada para processar cargas com poucos
recursos em longos períodos de tempo. Já a partição short é adequada
para processar cargas com mais recursos, porém, em um menor período de
tempo.</p>
<p>A partição long roda jobs por até 7 dias, onde cada job sofre preempção,
se houver jobs na fila, a partir de 2 dias. A partição short roda jobs
por até 2 dias, onde cada job sofre preempção, se houve jobs na fila, a
partir de 2 horas de execução do job. A preempção é uma suspensão
temporária que força jobs a voltarem para a fila. Os limites de CPU, MEM
e GPU para cada partição são apresentados na tabela abaixo.</p>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p>Nome</p></td>
<td><p>Tempo máximo</p></td>
<td><p>Preempção</p></td>
<td><p>CPU</p></td>
<td><p>MEM</p></td>
<td><p>GPU</p></td>
<td><p>Prioridade</p></td>
</tr>
<tr class="row-even"><td><p>long</p></td>
<td><p>7 dias</p></td>
<td><p>A partir de 2 dias de execução</p></td>
<td><p>16</p></td>
<td><p>32</p></td>
<td><p>1</p></td>
<td><p>100</p></td>
</tr>
<tr class="row-odd"><td><p>short</p></td>
<td><p>2 dias</p></td>
<td><p>A partir de 2 horas de execução</p></td>
<td><p>32</p></td>
<td><p>64</p></td>
<td><p>2</p></td>
<td><p>50</p></td>
</tr>
</tbody>
</table>
<p>Para rodar um script em uma determinada partição:</p>
<p>sbatch -p nome_particao –cpus-per-task n_cpus –mem=memoria
–gpus=n_gpus script.sh</p>
<p>Também é possível ajustar os limites de recursos no cabeçalho do
script.sh:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">#</span>!/bin/sh
<span class="gp">#</span>SBATCH<span class="w"> </span>--cpus-per-task<span class="o">=</span>n_cpus
<span class="gp">#</span>SBATCH<span class="w"> </span>--gpus<span class="o">=</span>n_gpus
<span class="gp">#</span>SBATCH<span class="w"> </span>--mem<span class="o">=</span>memoria
</pre></div>
</div>
</section>
<section id="politicas-de-priorizacao-de-jobs">
<span id="h-r8jgqwv0re1s"></span><span id="h-isb7a7i9a70r"></span><h1>Políticas de priorização de jobs<a class="headerlink" href="#politicas-de-priorizacao-de-jobs" title="Link permanente para este cabeçalho">¶</a></h1>
<p>Cada partição possui um fator de prioridade. A partição long
(prioridade=100) possui prioridade maior que a partição short
(prioridade=50). Além disto, como a quantidade de recursos pode variar
em cada job, considera-se o fator JobSize. Este fator prioriza jobs que
solicitam menos recursos computacionais. Considere dois usuários que
submetem jobs utilizando a partição ‘long’. O usuário A solicita X de
CPU e o usuário B solicita 2X de CPU. O usuário A, possui maior
prioridade</p>
<p>Portanto, para cada job é calculado um fator de prioridade de acordo com
a partição e recursos solicitados. Este fator de prioridade varia de 0.0
à 1.0. Por enquanto, considera-se dois fatores: Partition e JobSize.
Estes fatores possuem pesos iguais.</p>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="index.html">
              <img class="logo" src="_static/cin-logo.png" alt="Logo"/>
            </a></p>
  <div>
    <h3><a href="index.html">Tabela de Conteúdo</a></h3>
    <ul>
<li><a class="reference internal" href="#">Acessando o Cluster</a></li>
<li><a class="reference internal" href="#acessando-a-vpn-do-cin">Acessando a VPN do CIn</a></li>
<li><a class="reference internal" href="#conectando-a-maquina-de-acesso">Conectando a Maquina de Acesso</a></li>
<li><a class="reference internal" href="#ambientes-virtuais-python">Ambientes Virtuais Python</a></li>
<li><a class="reference internal" href="#pip-virtualenv">Pip/Virtualenv</a></li>
<li><a class="reference internal" href="#execucao-de-jobs">Execução de jobs</a><ul>
<li><a class="reference internal" href="#comandos-basicos-de-gerenciamento-de-jobs">Comandos básicos de gerenciamento de jobs</a></li>
<li><a class="reference internal" href="#exemplo-com-repositorio-publico-do-github">Exemplo com repositório público do GitHub</a></li>
<li><a class="reference internal" href="#exemplo-com-repositorio-privado-do-github">Exemplo com repositório privado do GitHub</a></li>
<li><a class="reference internal" href="#sincronizacao-de-arquivos-entre-o-cluster-a-sua-maquina">Sincronização de Arquivos Entre o Cluster a Sua Máquina</a></li>
</ul>
</li>
<li><a class="reference internal" href="#particoes-e-limites-de-recursos">Partições e limites de recursos</a></li>
<li><a class="reference internal" href="#politicas-de-priorizacao-de-jobs">Políticas de priorização de jobs</a></li>
</ul>

  </div>
  <div>
    <h4>Tópico anterior</h4>
    <p class="topless"><a href="cluster_cin.html"
                          title="capítulo anterior">Arquitetura do cluster Apuana</a></p>
  </div>
  <div>
    <h4>Próximo tópico</h4>
    <p class="topless"><a href="TheoryCluster/Theory_cluster.html"
                          title="próximo capítulo">O que é um cluster de computadores?</a></p>
  </div>
  <div role="note" aria-label="source link">
    <h3>Essa Página</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/usage.rst.txt"
            rel="nofollow">Exibir Fonte</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Busca rápida</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Ir" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navegação</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="Índice Geral"
             >índice</a></li>
        <li class="right" >
          <a href="TheoryCluster/Theory_cluster.html" title="O que é um cluster de computadores?"
             >próximo</a> |</li>
        <li class="right" >
          <a href="cluster_cin.html" title="Arquitetura do cluster Apuana"
             >anterior</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">documentação Cluster Cin latest</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Acessando o Cluster</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2023.
      Criada usando <a href="https://www.sphinx-doc.org/pt_BR/master">Sphinx</a> 5.3.0.
    </div>
  </body>
</html>