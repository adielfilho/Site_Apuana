
<!DOCTYPE html>

<html lang="pt-BR">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Acessando o Cluster &#8212; documentação Cluster Cin latest</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/groundwork.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/translations.js"></script>
    <link rel="index" title="Índice" href="../genindex.html" />
    <link rel="search" title="Buscar" href="../search.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navegação</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="Índice Geral"
             accesskey="I">índice</a></li>
        <li class="nav-item nav-item-0"><a href="../index.html">documentação Cluster Cin latest</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Acessando o Cluster</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="acessando-o-cluster">
<span id="h-js2b7t82mute"></span><h1>Acessando o Cluster<a class="headerlink" href="#acessando-o-cluster" title="Link permanente para este cabeçalho">¶</a></h1>
<p>Para acessar o cluster é necessário ter um login cin.ufpe.br e ter o
acesso habilitado as máquinas de acesso do cluster, também é necessário
estar na VPN do CIn.</p>
</section>
<section id="conectando-a-maquina-de-acesso">
<span id="h-krbkxr4n0rks"></span><h1>Conectando a Maquina de Acesso<a class="headerlink" href="#conectando-a-maquina-de-acesso" title="Link permanente para este cabeçalho">¶</a></h1>
<p>O envio de tarefas para o cluster precisa ser feito a partir de uma
maquina de acesso, a entrada na maquina de acesso é realizado através do
SSH</p>
<p>        ssh &lt;login&gt;&#64;slurm-client1.cin.ufpe.br</p>
<p>Uma vez conectado na máquina de acesso é possível alocar e editar seus
jobs a partir de comandos
<a class="reference external" href="https://www.google.com/url?q=https://slurm.schedmd.com/documentation.html&amp;sa=D&amp;source=editors&amp;ust=1683228690609179&amp;usg=AOvVaw2aSvDJwDiM7wf_WKq0h5lp">slurm</a>.
Primeiramente, você precisará preparar seu ambiente de trabalho no
servidor que você utilizará. Jobs que utilizam máquinas virtuais (Docker
e afins) não serão aprovados, pois abrem brechas de segurança. Qualquer
biblioteca que necessite de instalação, por favor, nos contate e faremos
a instalação o mais rápido possível. Para acessar o servidor que
realizará o processamento, utilize o comando “salloc”, como no exemplo
abaixo:</p>
<p>salloc</p>
<p>Nossos servidores já tem o gerenciador de pacotes
<a class="reference external" href="https://www.google.com/url?q=https://www.anaconda.com/products/distribution&amp;sa=D&amp;source=editors&amp;ust=1683228690610341&amp;usg=AOvVaw3cLVSQY5yUsKcLch94qn4r">conda</a> para
qualquer projeto que for realizado em Python. Os comandos de git também
estão disponíveis e prontos para uso. Após a preparação do seu ambiente,
volte para o nodo de login com o comando “exit” e rode o seu job com o
comando
<a class="reference external" href="https://www.google.com/url?q=https://slurm.schedmd.com/srun.html&amp;sa=D&amp;source=editors&amp;ust=1683228690610939&amp;usg=AOvVaw3QWOdTZiBB1V14-VrnC1a2">srun</a>.
Exemplo:</p>
<p>srun –gpus=N_GPUS –cpus-per-task=N_CPUS nvidia-smi</p>
</section>
<section id="ambientes-virtuais-python">
<span id="h-fnvohiub06p3"></span><h1>Ambientes Virtuais Python<a class="headerlink" href="#ambientes-virtuais-python" title="Link permanente para este cabeçalho">¶</a></h1>
<p>Um ambiente virtual em Python é um ambiente local e isolado no qual você
pode instalar ou desinstalar pacotes Python sem interferir no ambiente
global (ou em outros ambientes virtuais). Geralmente reside em um
diretório. Para usar um ambiente virtual, você deve ativá-lo. Ativar um
ambiente essencialmente define variáveis ​​de ambiente em seu shell para
que:</p>
<ul class="simple">
<li><p>‘python’ aponta para a versão correta do Python para aquele ambiente</p></li>
<li><p>‘python’ procura pelos pacotes no ambiente virual</p></li>
<li><p>‘pip install’ instala pacotes no ambiente virtual</p></li>
<li><p>Qualquer comando shell instalado via ‘pip install’ é disponibilizado</p></li>
</ul>
</section>
<section id="pip-virtualenv">
<span id="h-t03rdi3pn51o"></span><h1>Pip/Virtualenv<a class="headerlink" href="#pip-virtualenv" title="Link permanente para este cabeçalho">¶</a></h1>
<p>Pip é o gerenciador de pacotes preferido para Python e cada cluster
fornece várias versões do Python por meio do módulo associado que vem
com o pip. Para instalar novos pacotes, primeiro você terá que criar um
espaço pessoal para armazená-los. A solução preferida é usar ambientes
virtuais.</p>
<p>Primeiro, carregar o modulo Python que deseja utilizar</p>
<p> module load Python3.10</p>
<p>Modulos Disponiveis</p>
<ul class="simple">
<li><p>Python3.10 -&gt; Python/3.10.8</p></li>
<li><p>Python3.9 -&gt; Python/3.9.6</p></li>
<li><p>Python3.8 -&gt; Python/3.8.6</p></li>
</ul>
<p>Depois crie um ambiente virutal (onde &lt;env&gt; é o nome do ambiente) no seu
diretório home:</p>
<p>python -m venv $HOME/&lt;env&gt;</p>
<p>Para ativar o ambiente criado:</p>
<p>source $HOME/&lt;env&gt;/bin/activate</p>
<p>Você agora pode instalar qualquer pacote Python utilizando o comando
pip, exemplo pytorch:</p>
<p>pip install torch torchvision</p>
<p>Recomendamos que para a criação do ambiente para rodar o seus job,
alocar uma maquina de forma interativa utilizando o comando salloc, nele
você pode criar e testar o ambiente, uma vez criado e funcional, para
utilizar você deve carregar o modulo e ativar o ambiente no seu script.</p>
</section>
<section id="execucao-de-jobs">
<span id="h-elx4sixwhp8c"></span><h1>Execução de jobs<a class="headerlink" href="#execucao-de-jobs" title="Link permanente para este cabeçalho">¶</a></h1>
<p>Ao escrever um script .sh, é necessário ativar o ambiente no início e
instalar os pacotes necessários. Um exemplo de um script com tensorflow
é mostrado a seguir:</p>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p>#!/bin/bash</p>
<p>#SBATCH –job-name=test_job</p>
<p>#SBATCH –ntasks=1</p>
<p>#SBATCH –mem 16G</p>
<p>#SBATCH -c 8</p>
<p>#SBATCH -o job.log</p>
<p>#SBATCH –output=job_output.txt</p>
<p>#SBATCH –error=job_error.txt</p>
<p># carregar versão do python</p>
<p>module load Python/3.10</p>
<p># ativar ambiente</p>
<p>source $HOME/env_teste/bin/activate</p>
<p># executar .py</p>
<p>python $HOME/test_dir/test.py</p>
</td>
</tr>
</tbody>
</table>
<section id="comandos-basicos-de-gerenciamento-de-jobs">
<span id="h-h11648s05oky"></span><h2>Comandos básicos de gerenciamento de jobs<a class="headerlink" href="#comandos-basicos-de-gerenciamento-de-jobs" title="Link permanente para este cabeçalho">¶</a></h2>
<p>Para agendar o job faça:</p>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p>sbatch test_slurm.sh</p></td>
</tr>
</tbody>
</table>
<p>Para verificar erros no job faça (dentro do diretório do arquivo
job_error.txt):</p>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p>cat job_error.txt</p></td>
</tr>
</tbody>
</table>
<p>Para observar os outputs do job faça (dentro do diretório do arquivo
job_output.txt):</p>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p>cat job_output.txt</p></td>
</tr>
</tbody>
</table>
<p>Para verificar a posição do job na fila faça:</p>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p>squeue</p></td>
</tr>
</tbody>
</table>
<p>Para cancelar o job faça:</p>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p>scancel job_id</p></td>
</tr>
</tbody>
</table>
</section>
<section id="exemplo-com-repositorio-do-publico-do-github">
<span id="h-p9bbx0k61d7n"></span><span id="h-16ozdksheroh"></span><h2>Exemplo com repositório do público do GitHub<a class="headerlink" href="#exemplo-com-repositorio-do-publico-do-github" title="Link permanente para este cabeçalho">¶</a></h2>
<p>Primeiro é necessário clonar o repositório. Obs.: o diretório home do
usuário é sincronizado entre todas as máquinas.</p>
<p>git clone <a class="reference external" href="https://github.com/username/repoName.git">https://github.com/username/repoName.git</a></p>
<p>Depois de clonar o repositório, é criado um script .sh. Uma das
alternativas é utilizando nano:</p>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p>nano test_slurm.sh</p></td>
</tr>
</tbody>
</table>
<p>Em seguida, o usuário preenche o script com as diretivas do SBATCH que
ele acha necessário e depois com os comandos que devem ser executados no
node. Primeiro, será apresentado um script que faz uso de Pytorch.</p>
<p>test_slurm.sh</p>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p>#!/bin/bash</p>
<p>#SBATCH –job-name=test_job</p>
<p>#SBATCH –ntasks=1</p>
<p>#SBATCH –mem 16G</p>
<p>#SBATCH -c 8</p>
<p>#SBATCH -o job.log</p>
<p>#SBATCH –output=job_output.txt</p>
<p>#SBATCH –error=job_error.txt</p>
<p># carregar versão do python</p>
<p>module load Python/3.9.6</p>
<p># criar ambiente</p>
<p>python -m venv $HOME/env_teste</p>
<p># ativar ambiente</p>
<p>source $HOME/env_teste/bin/activate</p>
<p># instalar pacotes desejados
pip install pytorch
pip install pandas
pip install matplotlib
pip install seaborn
Pip install IPython</p>
<p># executar .py
python $HOME/repoName/thisScript.py</p>
</td>
</tr>
</tbody>
</table>
<p>Perceba a criação de um novo ambiente e em seguida sua ativação. Aqui
foi realizado o dowgrade da versão do Python de 3.10 para 3.9. Isto foi
feito porque ainda existem bugs da classe DataLoader do PyTorch ao
utilizar o python 3.10. O cluster é bastante versátil neste aspecto,
pois pode-se escolher a versão do python (dentre as listadas acima) e
das dependências mais adequadas para o funcionamento do seu código no
ambiente virtual.</p>
</section>
<section id="exemplo-com-repositorio-do-privado-do-github">
<span id="h-u2s1lukw4mc7"></span><h2>Exemplo com repositório do privado do GitHub<a class="headerlink" href="#exemplo-com-repositorio-do-privado-do-github" title="Link permanente para este cabeçalho">¶</a></h2>
<p>Clonando um repositório privado :</p>
<p>Para clone repositorios privados recomendamos conectar na maquina de
login (slurm-client1) passando adiante o seu agente SSH, permitindo a
você utilizar a chave SSH configurada na sua maquina local na sessão
SSH, para tal é preciso utilizar o parametro -A</p>
<p>ssh -A &lt;login&gt;&#64;slurm-client1.cin.ufpe.br</p>
<p>git clone <a class="reference external" href="mailto:git&#37;&#52;&#48;github&#46;com">git<span>&#64;</span>github<span>&#46;</span>com</a>:username:token&#64;github.com/username/repoName.git</p>
</section>
<section id="sincronizacao-de-arquivos-entre-o-cluster-a-sua-maquina">
<span id="h-kbo1jo3m9z40"></span><h2>Sincronização de Arquivos Entre o Cluster a Sua Máquina<a class="headerlink" href="#sincronizacao-de-arquivos-entre-o-cluster-a-sua-maquina" title="Link permanente para este cabeçalho">¶</a></h2>
<p>Para a sincronização/transferência de arquivos entre sua máquina e o
cluster deve ser utilizado o comando rsync
(<a class="reference external" href="https://www.google.com/url?q=https://download.samba.org/pub/rsync/rsync.1&amp;sa=D&amp;source=editors&amp;ust=1683228690625949&amp;usg=AOvVaw1h4cJRq-mEcdnOojWx8lGE">Documentação</a>,
<a class="reference external" href="https://www.google.com/url?q=https://www.digitalocean.com/community/tutorials/how-to-use-rsync-to-sync-local-and-remote-directories-pt&amp;sa=D&amp;source=editors&amp;ust=1683228690626302&amp;usg=AOvVaw2x91BoV8AGqpGJzdYZulSa">Tutorial</a>)
através da diretório home da máquina de login, lembrando que este é
sincronizado com os nós de computação do cluster.</p>
<p>Exemplos:</p>
<ul class="simple">
<li><p>Sincronizando pasta da máquina local para o cluster:
rsync –bwlimit=1000 -azP pasta-1 &lt;login&gt;&#64;slurm-client1.cin.ufpe.br:~</p></li>
<li><p>A pasta de nome “pasta-1” vai ser copiada/sincronizada para a pasta
de nome “pasta-1” no seu diretório home “~/pasta-1”</p></li>
<li><p>O argumento –bwlimit limita a velocidade de transferência, dado em
KBytes por segund</p></li>
<li><p>O argumento -a é para archive mode, sendo equivalente a utilização
dos argumentos “ -rlptgoD “</p></li>
<li><p>O argumento -z realiza a compressão dos dados para transferência</p></li>
<li><p>O argumento -P mostra o progresso da transferência e resume
transferências interrompidas</p></li>
<li><p>Sincronizando pasta do cluster para máquina local:
rsync –bwlimit=1000 -azP &lt;login&gt;&#64;slurm-client1.cin.ufpe.br:~/pasta-1
~</p></li>
<li><p>Mesmo do exemplo anterior, porém a ordem é invertida, a pasta-1 do
diretório home do cluster é copiada para o diretório home local</p></li>
</ul>
<p>A utilização do rsync também é recomendada para transferências dentro da
própria máquina do cluster quando for realizada entre volumes em rede,
como por exemplo transferências do diretório home para o diretório /tmp
local do nó de computação</p>
</section>
<section id="particoes-e-limites-de-recursos">
<span id="h-6xrj2d7u8h14"></span><h2>Partições e limites de recursos<a class="headerlink" href="#particoes-e-limites-de-recursos" title="Link permanente para este cabeçalho">¶</a></h2>
<p>Atualmente o cluster contém duas partições: long e short. A partição
long é a partição padrão e  é adequada para processar cargas com poucos
recursos em longos períodos de tempo. Já a partição short é adequada
para processar cargas com mais recursos, porém, em um menor período de
tempo.</p>
<p>A partição long roda jobs por até 7 dias, onde cada job sofre preempção,
se houver jobs na fila, a partir de 2 dias. A partição short roda jobs
por até 2 dias, onde cada job sofre preempção, se houve jobs na fila, a
partir de 2 horas de execução do job. A preempção é uma suspensão
temporária que força jobs a voltarem para a fila. Os limites de CPU, MEM
e GPU para cada partição são apresentados na tabela abaixo.</p>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p>Nome</p></td>
<td><p>Tempo máximo</p></td>
<td><p>Preempção</p></td>
<td><p>CPU</p></td>
<td><p>MEM</p></td>
<td><p>GPU</p></td>
<td><p>Prioridade</p></td>
</tr>
<tr class="row-even"><td><p>long</p></td>
<td><p>7 dias</p></td>
<td><p>A partir de 2 dias de execução</p></td>
<td><p>16</p></td>
<td><p>32</p></td>
<td><p>1</p></td>
<td><p>100</p></td>
</tr>
<tr class="row-odd"><td><p>short</p></td>
<td><p>2 dias</p></td>
<td><p>A partir de 2 horas de execução</p></td>
<td><p>32</p></td>
<td><p>64</p></td>
<td><p>2</p></td>
<td><p>50</p></td>
</tr>
</tbody>
</table>
<p>Para rodar um script em uma determinada partição:</p>
<p>sbatch -p nome_particao –cpus-per-task n_cpus –mem=memoria
–gpus=n_gpus script.sh</p>
<p>Também é possível ajustar os limites de recursos no cabeçalho do
script.sh:</p>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p>#!/bin/sh
#SBATCH –cpus-per-task=n_cpus</p>
<p>#SBATCH –gpus=n_gpus</p>
<p>#SBATCH –mem=memoria</p>
</td>
</tr>
</tbody>
</table>
</section>
<section id="politicas-de-priorizacao-de-jobs">
<span id="h-r8jgqwv0re1s"></span><span id="h-isb7a7i9a70r"></span><h2>Políticas de priorização de jobs<a class="headerlink" href="#politicas-de-priorizacao-de-jobs" title="Link permanente para este cabeçalho">¶</a></h2>
<p>Cada partição possui um fator de prioridade. A partição long
(prioridade=100) possui prioridade maior que a partição short
(prioridade=50). Além disto, como a quantidade de recursos pode variar
em cada job, considera-se o fator JobSize. Este fator prioriza jobs que
solicitam menos recursos computacionais. Considere dois usuários que
submetem jobs utilizando a partição ‘long’. O usuário A solicita X de
CPU e o usuário B solicita 2X de CPU. O usuário A, possui maior
prioridade</p>
<p>Portanto, para cada job é calculado um fator de prioridade de acordo com
a partição e recursos solicitados. Este fator de prioridade varia de 0.0
à 1.0. Por enquanto, considera-se dois fatores: Partition e JobSize.
Estes fatores possuem pesos iguais.</p>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../index.html">
              <img class="logo" src="../_static/cin-logo.png" alt="Logo"/>
            </a></p>
  <div>
    <h3><a href="../index.html">Tabela de Conteúdo</a></h3>
    <ul>
<li><a class="reference internal" href="#">Acessando o Cluster</a></li>
<li><a class="reference internal" href="#conectando-a-maquina-de-acesso">Conectando a Maquina de Acesso</a></li>
<li><a class="reference internal" href="#ambientes-virtuais-python">Ambientes Virtuais Python</a></li>
<li><a class="reference internal" href="#pip-virtualenv">Pip/Virtualenv</a></li>
<li><a class="reference internal" href="#execucao-de-jobs">Execução de jobs</a><ul>
<li><a class="reference internal" href="#comandos-basicos-de-gerenciamento-de-jobs">Comandos básicos de gerenciamento de jobs</a></li>
<li><a class="reference internal" href="#exemplo-com-repositorio-do-publico-do-github">Exemplo com repositório do público do GitHub</a></li>
<li><a class="reference internal" href="#exemplo-com-repositorio-do-privado-do-github">Exemplo com repositório do privado do GitHub</a></li>
<li><a class="reference internal" href="#sincronizacao-de-arquivos-entre-o-cluster-a-sua-maquina">Sincronização de Arquivos Entre o Cluster a Sua Máquina</a></li>
<li><a class="reference internal" href="#particoes-e-limites-de-recursos">Partições e limites de recursos</a></li>
<li><a class="reference internal" href="#politicas-de-priorizacao-de-jobs">Políticas de priorização de jobs</a></li>
</ul>
</li>
</ul>

  </div>
  <div role="note" aria-label="source link">
    <h3>Essa Página</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/maintain/usage.rst.txt"
            rel="nofollow">Exibir Fonte</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Busca rápida</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Ir" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navegação</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="Índice Geral"
             >índice</a></li>
        <li class="nav-item nav-item-0"><a href="../index.html">documentação Cluster Cin latest</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Acessando o Cluster</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2023.
      Criada usando <a href="https://www.sphinx-doc.org/pt_BR/master">Sphinx</a> 5.3.0.
    </div>
  </body>
</html>